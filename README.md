# Rfy23/qwenvl-7B-medical-ko-zh

**ë³¸ í”„ë¡œì íŠ¸ëŠ” êµ­ë‚´ ì˜ë£Œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ì™¸êµ­ì¸ì„ ìœ„í•œ ì˜ë£Œ ë¬¸ì„œ ì¸ì‹ AI ëª¨ë¸ì…ë‹ˆë‹¤. Vision Language Model(VLM)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ì˜ ì²˜ë°©ì „, ê±´ê°•ê²€ì§„ ê²°ê³¼ì§€, ì§„ë£Œë¹„ ì˜ìˆ˜ì¦ ë“± ë³µì¡í•œ ì˜ë£Œ ë¬¸ì„œë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ê³  ì¸ì‹í•©ë‹ˆë‹¤.**

<img src="image/main_ai.png">

## ğŸŒ Model Fine-tuning
base model: [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)
ğŸ¤— fine-tuning model: [qwenvl-7B-medical-ko-zh](https://huggingface.co/Rfy23/qwenvl-7B-medical-ko-zh)

íš¨ìœ¨ì ì¸ ìì› ì‚¬ìš©ê³¼ ì •í™•í•œ OCR ì„±ëŠ¥ì„ ìœ„í•´ Hybrid Fine-tuning ì „ëµì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤.

- Vision Tower & Merger (Full Fine-tuning): ì²˜ë°©ì „ì˜ ë¯¸ì„¸í•œ í•œê¸€ íš(ã…—, ã…œ ë“±) ì¸ì‹ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì–¸í”„ë¦¬ì§•(Unfrozen)í•˜ì—¬ ì§ì ‘ í•™ìŠµ.
- LLM (LoRA): ëª¨ë¸ ë³¸ì²´ ê°€ì¤‘ì¹˜ëŠ” ë™ê²°(Frozen)í•˜ê³ , í•µì‹¬ ì–´í…ì…˜ ë ˆì´ì–´(q_proj, v_proj)ì— LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´ ë° ê¸°ì¡´ ì–¸ì–´ ì§€ì‹ ë³´ì¡´.
---
## ğŸŒ Files Structure

```text
.
â”œâ”€â”€ data/                # í•©ì„± ì²˜ë°©ì „ ë° ì˜ìˆ˜ì¦ ë°ì´í„°ì…‹ (.json ë° ì´ë¯¸ì§€ íŒŒì¼)
â”œâ”€â”€ scripts/             # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ë¥¼ ìœ„í•œ Shell ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ finetune_lora.sh
â”œâ”€â”€ src/                 # ë©”ì¸ ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ dataset/         # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ë¡œì§ (VLM í˜•ì‹ ë³€í™˜)
â”‚   â”œâ”€â”€ loss/            # í•™ìŠµì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
â”‚   â”œâ”€â”€ model/           # Qwen2-VL ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ì„¤ì • ê´€ë ¨ ì½”ë“œ
â”‚   â”œâ”€â”€ serve/           # ì¶”ë¡ (Inference) ë° API ì„œë¹™ ê´€ë ¨ ì½”ë“œ
â”‚   â”œâ”€â”€ train/           # SFT(Supervised Fine-Tuning) ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ trainer/         # íŒŒì´í† ì¹˜/DeepSpeed ê¸°ë°˜ í•™ìŠµ ì—”ì§„ ê´€ë¦¬
â””â”€â”€ output/              # ì²´í¬í¬ì¸íŠ¸ ë° í•™ìŠµ ë¡œê·¸ ì €ì¥ í´ë” â¡ï¸ .gitignore
```
---

## ğŸŒ Dataset Info
<img src="image/datasetsInfo.png">

- Train Data (3,636 samples): í•©ì„±ëœ í•œêµ­ì–´ ì²˜ë°©ì „(90%) + Key-Value í•™ìŠµìš© ì˜ìˆ˜ì¦(10)
- Test Data (481 samples): ì‹¤ì œ ì²˜ë°©ì „ ì–‘ì‹ ê¸°ë°˜ í…ŒìŠ¤íŠ¸ì…‹

ë°ì´í„°ì…‹ì€ [ì—¬ê¸°](https://github.com/Saeroi-an/AI/tree/main/data)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”í›„ì— í—ˆê¹…í˜ì´ìŠ¤ì— ì—…ë¡œë“œ í•  ì˜ˆì •ì…ë‹ˆë‹¤.



---

## ğŸŒ How to Train
**requirements.txt ì„¤ì¹˜**
```bash
pip install -r requirements.txt
```

**lora fine-tuning ì‹¤í–‰**
```bash
cd scripts
bash finetune_lora.sh
```

**ì£¼ìš” íŒŒë¼ë¯¸í„° ì„¤ì •**
- Precision: bf16
- Optimization: DeepSpeed ZeRO-3 Offload, Liger Kernel
- Learning Rate: LLM($5\times10^{-6}$), Vision($2\times10^{-6}$), Merger($1\times10^{-5}$)
- Batch Size: 32 (Global) / Epochs: 5

  
