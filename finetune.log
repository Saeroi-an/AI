[2025-10-14 20:10:53,610] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 20:10:55,752] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-14 20:10:56,916] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2025-10-14 20:10:56,916] [INFO] [runner.py:610:main] cmd = /home/jwlee/miniconda3/envs/train/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py --use_liger True --lora_enable True --use_dora False --lora_namespan_exclude ['lm_head', 'embed_tokens'] --lora_rank 32 --lora_alpha 32 --lora_dropout 0.05 --num_lora_modules -1 --deepspeed /home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/zero3_offload.json --model_id Qwen/Qwen2-VL-2B-Instruct --data_path /home/jwlee/volume/Qwen2-vl-finetune-wo/synth_rx/train_cord.json --image_folder /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images --remove_unused_columns False --freeze_vision_tower False --freeze_llm True --freeze_merger False --bf16 True --fp16 False --disable_flash_attn2 False --output_dir /home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord --num_train_epochs 2 --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --image_min_pixels 200704 --image_max_pixels 1097600 --learning_rate 1e-4 --merger_lr 1e-5 --vision_lr 2e-6 --weight_decay 0.1 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --gradient_checkpointing True --report_to tensorboard --lazy_preprocess True --save_strategy steps --save_steps 50 --save_total_limit 5 --dataloader_num_workers 2
[2025-10-14 20:10:58,148] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 20:11:00,577] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-14 20:11:01,733] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-14 20:11:01,734] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-14 20:11:01,734] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-14 20:11:01,734] [INFO] [launch.py:164:main] dist_world_size=2
[2025-10-14 20:11:01,734] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-14 20:11:01,734] [INFO] [launch.py:256:main] process 107471 spawned with command: ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', '/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py', '--local_rank=0', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', '/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', '/home/jwlee/volume/Qwen2-vl-finetune-wo/synth_rx/train_cord.json', '--image_folder', '/home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2']
[2025-10-14 20:11:01,735] [INFO] [launch.py:256:main] process 107472 spawned with command: ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', '/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py', '--local_rank=1', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', '/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', '/home/jwlee/volume/Qwen2-vl-finetune-wo/synth_rx/train_cord.json', '--image_folder', '/home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2']
[2025-10-14 20:11:05,266] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 20:11:05,508] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-14 20:11:06,501] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-14 20:11:06,510] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-14 20:11:06,913] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-14 20:11:06,922] [INFO] [comm.py:821:init_distributed] cdb=None
[2025-10-14 20:11:06,922] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-10-14 20:11:08,177] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-10-14 20:11:08,181] [INFO] [config.py:684:__init__] Config mesh_device None world_size = 2
[2025-10-14 20:11:15,198] [INFO] [partition_parameters.py:366:__exit__] finished initializing model - num_params = 730, num_elems = 2.44B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]
Found 196 lora modules: ['model.language_model.layers.0.self_attn.q_proj', 'model.language_model.layers.0.self_attn.k_proj', 'model.language_model.layers.0.self_attn.v_proj', 'model.language_model.layers.0.self_attn.o_proj', 'model.language_model.layers.0.mlp.gate_proj', 'model.language_model.layers.0.mlp.up_proj', 'model.language_model.layers.0.mlp.down_proj', 'model.language_model.layers.1.self_attn.q_proj', 'model.language_model.layers.1.self_attn.k_proj', 'model.language_model.layers.1.self_attn.v_proj', 'model.language_model.layers.1.self_attn.o_proj', 'model.language_model.layers.1.mlp.gate_proj', 'model.language_model.layers.1.mlp.up_proj', 'model.language_model.layers.1.mlp.down_proj', 'model.language_model.layers.2.self_attn.q_proj', 'model.language_model.layers.2.self_attn.k_proj', 'model.language_model.layers.2.self_attn.v_proj', 'model.language_model.layers.2.self_attn.o_proj', 'model.language_model.layers.2.mlp.gate_proj', 'model.language_model.layers.2.mlp.up_proj', 'model.language_model.layers.2.mlp.down_proj', 'model.language_model.layers.3.self_attn.q_proj', 'model.language_model.layers.3.self_attn.k_proj', 'model.language_model.layers.3.self_attn.v_proj', 'model.language_model.layers.3.self_attn.o_proj', 'model.language_model.layers.3.mlp.gate_proj', 'model.language_model.layers.3.mlp.up_proj', 'model.language_model.layers.3.mlp.down_proj', 'model.language_model.layers.4.self_attn.q_proj', 'model.language_model.layers.4.self_attn.k_proj', 'model.language_model.layers.4.self_attn.v_proj', 'model.language_model.layers.4.self_attn.o_proj', 'model.language_model.layers.4.mlp.gate_proj', 'model.language_model.layers.4.mlp.up_proj', 'model.language_model.layers.4.mlp.down_proj', 'model.language_model.layers.5.self_attn.q_proj', 'model.language_model.layers.5.self_attn.k_proj', 'model.language_model.layers.5.self_attn.v_proj', 'model.language_model.layers.5.self_attn.o_proj', 'model.language_model.layers.5.mlp.gate_proj', 'model.language_model.layers.5.mlp.up_proj', 'model.language_model.layers.5.mlp.down_proj', 'model.language_model.layers.6.self_attn.q_proj', 'model.language_model.layers.6.self_attn.k_proj', 'model.language_model.layers.6.self_attn.v_proj', 'model.language_model.layers.6.self_attn.o_proj', 'model.language_model.layers.6.mlp.gate_proj', 'model.language_model.layers.6.mlp.up_proj', 'model.language_model.layers.6.mlp.down_proj', 'model.language_model.layers.7.self_attn.q_proj', 'model.language_model.layers.7.self_attn.k_proj', 'model.language_model.layers.7.self_attn.v_proj', 'model.language_model.layers.7.self_attn.o_proj', 'model.language_model.layers.7.mlp.gate_proj', 'model.language_model.layers.7.mlp.up_proj', 'model.language_model.layers.7.mlp.down_proj', 'model.language_model.layers.8.self_attn.q_proj', 'model.language_model.layers.8.self_attn.k_proj', 'model.language_model.layers.8.self_attn.v_proj', 'model.language_model.layers.8.self_attn.o_proj', 'model.language_model.layers.8.mlp.gate_proj', 'model.language_model.layers.8.mlp.up_proj', 'model.language_model.layers.8.mlp.down_proj', 'model.language_model.layers.9.self_attn.q_proj', 'model.language_model.layers.9.self_attn.k_proj', 'model.language_model.layers.9.self_attn.v_proj', 'model.language_model.layers.9.self_attn.o_proj', 'model.language_model.layers.9.mlp.gate_proj', 'model.language_model.layers.9.mlp.up_proj', 'model.language_model.layers.9.mlp.down_proj', 'model.language_model.layers.10.self_attn.q_proj', 'model.language_model.layers.10.self_attn.k_proj', 'model.language_model.layers.10.self_attn.v_proj', 'model.language_model.layers.10.self_attn.o_proj', 'model.language_model.layers.10.mlp.gate_proj', 'model.language_model.layers.10.mlp.up_proj', 'model.language_model.layers.10.mlp.down_proj', 'model.language_model.layers.11.self_attn.q_proj', 'model.language_model.layers.11.self_attn.k_proj', 'model.language_model.layers.11.self_attn.v_proj', 'model.language_model.layers.11.self_attn.o_proj', 'model.language_model.layers.11.mlp.gate_proj', 'model.language_model.layers.11.mlp.up_proj', 'model.language_model.layers.11.mlp.down_proj', 'model.language_model.layers.12.self_attn.q_proj', 'model.language_model.layers.12.self_attn.k_proj', 'model.language_model.layers.12.self_attn.v_proj', 'model.language_model.layers.12.self_attn.o_proj', 'model.language_model.layers.12.mlp.gate_proj', 'model.language_model.layers.12.mlp.up_proj', 'model.language_model.layers.12.mlp.down_proj', 'model.language_model.layers.13.self_attn.q_proj', 'model.language_model.layers.13.self_attn.k_proj', 'model.language_model.layers.13.self_attn.v_proj', 'model.language_model.layers.13.self_attn.o_proj', 'model.language_model.layers.13.mlp.gate_proj', 'model.language_model.layers.13.mlp.up_proj', 'model.language_model.layers.13.mlp.down_proj', 'model.language_model.layers.14.self_attn.q_proj', 'model.language_model.layers.14.self_attn.k_proj', 'model.language_model.layers.14.self_attn.v_proj', 'model.language_model.layers.14.self_attn.o_proj', 'model.language_model.layers.14.mlp.gate_proj', 'model.language_model.layers.14.mlp.up_proj', 'model.language_model.layers.14.mlp.down_proj', 'model.language_model.layers.15.self_attn.q_proj', 'model.language_model.layers.15.self_attn.k_proj', 'model.language_model.layers.15.self_attn.v_proj', 'model.language_model.layers.15.self_attn.o_proj', 'model.language_model.layers.15.mlp.gate_proj', 'model.language_model.layers.15.mlp.up_proj', 'model.language_model.layers.15.mlp.down_proj', 'model.language_model.layers.16.self_attn.q_proj', 'model.language_model.layers.16.self_attn.k_proj', 'model.language_model.layers.16.self_attn.v_proj', 'model.language_model.layers.16.self_attn.o_proj', 'model.language_model.layers.16.mlp.gate_proj', 'model.language_model.layers.16.mlp.up_proj', 'model.language_model.layers.16.mlp.down_proj', 'model.language_model.layers.17.self_attn.q_proj', 'model.language_model.layers.17.self_attn.k_proj', 'model.language_model.layers.17.self_attn.v_proj', 'model.language_model.layers.17.self_attn.o_proj', 'model.language_model.layers.17.mlp.gate_proj', 'model.language_model.layers.17.mlp.up_proj', 'model.language_model.layers.17.mlp.down_proj', 'model.language_model.layers.18.self_attn.q_proj', 'model.language_model.layers.18.self_attn.k_proj', 'model.language_model.layers.18.self_attn.v_proj', 'model.language_model.layers.18.self_attn.o_proj', 'model.language_model.layers.18.mlp.gate_proj', 'model.language_model.layers.18.mlp.up_proj', 'model.language_model.layers.18.mlp.down_proj', 'model.language_model.layers.19.self_attn.q_proj', 'model.language_model.layers.19.self_attn.k_proj', 'model.language_model.layers.19.self_attn.v_proj', 'model.language_model.layers.19.self_attn.o_proj', 'model.language_model.layers.19.mlp.gate_proj', 'model.language_model.layers.19.mlp.up_proj', 'model.language_model.layers.19.mlp.down_proj', 'model.language_model.layers.20.self_attn.q_proj', 'model.language_model.layers.20.self_attn.k_proj', 'model.language_model.layers.20.self_attn.v_proj', 'model.language_model.layers.20.self_attn.o_proj', 'model.language_model.layers.20.mlp.gate_proj', 'model.language_model.layers.20.mlp.up_proj', 'model.language_model.layers.20.mlp.down_proj', 'model.language_model.layers.21.self_attn.q_proj', 'model.language_model.layers.21.self_attn.k_proj', 'model.language_model.layers.21.self_attn.v_proj', 'model.language_model.layers.21.self_attn.o_proj', 'model.language_model.layers.21.mlp.gate_proj', 'model.language_model.layers.21.mlp.up_proj', 'model.language_model.layers.21.mlp.down_proj', 'model.language_model.layers.22.self_attn.q_proj', 'model.language_model.layers.22.self_attn.k_proj', 'model.language_model.layers.22.self_attn.v_proj', 'model.language_model.layers.22.self_attn.o_proj', 'model.language_model.layers.22.mlp.gate_proj', 'model.language_model.layers.22.mlp.up_proj', 'model.language_model.layers.22.mlp.down_proj', 'model.language_model.layers.23.self_attn.q_proj', 'model.language_model.layers.23.self_attn.k_proj', 'model.language_model.layers.23.self_attn.v_proj', 'model.language_model.layers.23.self_attn.o_proj', 'model.language_model.layers.23.mlp.gate_proj', 'model.language_model.layers.23.mlp.up_proj', 'model.language_model.layers.23.mlp.down_proj', 'model.language_model.layers.24.self_attn.q_proj', 'model.language_model.layers.24.self_attn.k_proj', 'model.language_model.layers.24.self_attn.v_proj', 'model.language_model.layers.24.self_attn.o_proj', 'model.language_model.layers.24.mlp.gate_proj', 'model.language_model.layers.24.mlp.up_proj', 'model.language_model.layers.24.mlp.down_proj', 'model.language_model.layers.25.self_attn.q_proj', 'model.language_model.layers.25.self_attn.k_proj', 'model.language_model.layers.25.self_attn.v_proj', 'model.language_model.layers.25.self_attn.o_proj', 'model.language_model.layers.25.mlp.gate_proj', 'model.language_model.layers.25.mlp.up_proj', 'model.language_model.layers.25.mlp.down_proj', 'model.language_model.layers.26.self_attn.q_proj', 'model.language_model.layers.26.self_attn.k_proj', 'model.language_model.layers.26.self_attn.v_proj', 'model.language_model.layers.26.self_attn.o_proj', 'model.language_model.layers.26.mlp.gate_proj', 'model.language_model.layers.26.mlp.up_proj', 'model.language_model.layers.26.mlp.down_proj', 'model.language_model.layers.27.self_attn.q_proj', 'model.language_model.layers.27.self_attn.k_proj', 'model.language_model.layers.27.self_attn.v_proj', 'model.language_model.layers.27.self_attn.o_proj', 'model.language_model.layers.27.mlp.gate_proj', 'model.language_model.layers.27.mlp.up_proj', 'model.language_model.layers.27.mlp.down_proj']
Adding LoRA to the model...
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 10058.28it/s]
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 9362.29it/s]
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 12228.29it/s]
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 2125.85it/s]
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 3175.10it/s]
Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 2312.19it/s]
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 8. Using DeepSpeed's value.
Installed CUDA version 12.2 does not match the version torch was compiled with 12.8 but since the APIs are compatible, accepting this combination
[rank1]:W1014 20:11:33.917000 107472 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank1]:W1014 20:11:33.917000 107472 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
ninja: no work to do.
Time to load cpu_adam op: 2.254459857940674 seconds
Installed CUDA version 12.2 does not match the version torch was compiled with 12.8 but since the APIs are compatible, accepting this combination
[rank0]:W1014 20:11:33.949000 107471 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank0]:W1014 20:11:33.949000 107471 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
ninja: no work to do.
Time to load cpu_adam op: 2.2681994438171387 seconds
Parameter Offload - Persistent parameters statistics: param_count = 457, numel = 1145344
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00290.jpg
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00410.jpg
[SUCCESS] get_image_info returned PIL image with size: (588, 868)
[SUCCESS] get_image_info returned PIL image with size: (840, 1260)
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00189.jpg
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00490.jpg
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py", line 265, in <module>
[rank1]:     train()
[rank1]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py", line 239, in train
[rank1]:     trainer.train()
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 2328, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 2623, in _inner_training_loop
[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
[rank1]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 5581, in get_batch_samples
[rank1]:     batch_samples.append(next(epoch_iterator))
[rank1]:                          ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/accelerate/data_loader.py", line 567, in __iter__
[rank1]:     current_batch = next(dataloader_iter)
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank1]:     data = self._next_data()
[rank1]:            ^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1516, in _next_data
[rank1]:     return self._process_data(data, worker_id)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1551, in _process_data
[rank1]:     data.reraise()
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/_utils.py", line 769, in reraise
[rank1]:     raise exception
[rank1]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank1]: Original Traceback (most recent call last):
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
[rank1]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank1]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank1]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank1]:             ~~~~~~~~~~~~^^^^^
[rank1]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/dataset/sft_dataset.py", line 155, in __getitem__
[rank1]:     inputs = processor(text=[user_input], images=images_for_this_turn, videos=videos, padding=False, do_resize=False, return_tensors='pt')
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py", line 144, in __call__
[rank1]:     image_inputs = self.image_processor(images=images, **output_kwargs["images_kwargs"])
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py", line 637, in __call__
[rank1]:     return self.preprocess(images, *args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 151, in preprocess
[rank1]:     return super().preprocess(images, videos, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py", line 662, in preprocess
[rank1]:     return self._preprocess_image_like_inputs(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 173, in _preprocess_image_like_inputs
[rank1]:     batch_feature = self._preprocess(images, **kwargs)
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 211, in _preprocess
[rank1]:     grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)
[rank1]:                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_transforms.py", line 917, in group_images_by_shape
[rank1]:     device = images[0][0].device if is_nested else images[0].device
[rank1]:                                                    ~~~~~~^^^
[rank1]: IndexError: list index out of range

[SUCCESS] get_image_info returned PIL image with size: (728, 1288)
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00132.jpg
[SUCCESS] get_image_info returned PIL image with size: (784, 1372)
[SUCCESS] get_image_info returned PIL image with size: (784, 1372)
  0%|          | 0/50 [00:00<?, ?it/s][INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00042.jpg
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00210.jpg
[SUCCESS] get_image_info returned PIL image with size: (588, 868)
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00271.jpg
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py", line 265, in <module>
[rank0]:     train()
[rank0]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py", line 239, in train
[rank0]:     trainer.train()
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 2328, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 2623, in _inner_training_loop
[rank0]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)
[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/trainer.py", line 5581, in get_batch_samples
[rank0]:     batch_samples.append(next(epoch_iterator))
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/accelerate/data_loader.py", line 567, in __iter__
[rank0]:     current_batch = next(dataloader_iter)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 734, in __next__
[rank0]:     data = self._next_data()
[rank0]:            ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1516, in _next_data
[rank0]:     return self._process_data(data, worker_id)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1551, in _process_data
[rank0]:     data.reraise()
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/_utils.py", line 769, in reraise
[rank0]:     raise exception
[rank0]: IndexError: Caught IndexError in DataLoader worker process 0.
[rank0]: Original Traceback (most recent call last):
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 349, in _worker_loop
[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
[rank0]:     data = [self.dataset[idx] for idx in possibly_batched_index]
[rank0]:             ~~~~~~~~~~~~^^^^^
[rank0]:   File "/home/jwlee/volume/Qwen2-vl-finetune-wo/src/dataset/sft_dataset.py", line 155, in __getitem__
[rank0]:     inputs = processor(text=[user_input], images=images_for_this_turn, videos=videos, padding=False, do_resize=False, return_tensors='pt')
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/processing_qwen2_vl.py", line 144, in __call__
[rank0]:     image_inputs = self.image_processor(images=images, **output_kwargs["images_kwargs"])
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py", line 637, in __call__
[rank0]:     return self.preprocess(images, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 151, in preprocess
[rank0]:     return super().preprocess(images, videos, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_processing_utils_fast.py", line 662, in preprocess
[rank0]:     return self._preprocess_image_like_inputs(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 173, in _preprocess_image_like_inputs
[rank0]:     batch_feature = self._preprocess(images, **kwargs)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py", line 211, in _preprocess
[rank0]:     grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)
[rank0]:                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/jwlee/miniconda3/envs/train/lib/python3.11/site-packages/transformers/image_transforms.py", line 917, in group_images_by_shape
[rank0]:     device = images[0][0].device if is_nested else images[0].device
[rank0]:                                                    ~~~~~~^^^
[rank0]: IndexError: list index out of range

[SUCCESS] get_image_info returned PIL image with size: (840, 1260)
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00456.jpg
[SUCCESS] get_image_info returned PIL image with size: (784, 1372)
[SUCCESS] get_image_info returned PIL image with size: (840, 1260)
[INFO] Processing image 1/1: /home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images/00451.jpg
[SUCCESS] get_image_info returned PIL image with size: (896, 1204)
[2025-10-14 20:11:49,741] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 107471
[2025-10-14 20:11:49,741] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 107472
[2025-10-14 20:11:49,757] [ERROR] [launch.py:325:sigkill_handler] ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', '/home/jwlee/volume/Qwen2-vl-finetune-wo/src/train/train_sft.py', '--local_rank=1', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', '/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', '/home/jwlee/volume/Qwen2-vl-finetune-wo/synth_rx/train_cord.json', '--image_folder', '/home/jwlee/volume/Qwen2-vl-finetune-wo/data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2'] exits with return code = 1
