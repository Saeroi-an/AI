import os
import getpass
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_core.tools import tool
from langchain.agents import create_tool_calling_agent, AgentExecutor

if not os.getenv("HUGGINGFACEHUB_API_TOKEN"):
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = getpass.getpass("Enter your token: ")
    
    
HF_REPO_ID = "skt/ko-gpt-trinity-1.2B-v0.5"
MODEL_NAME = "Rfy23/qwen2vl-ko-zh"
IMAGE_URL = "/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/test-Img/images/00003.jpg"
FIXED_QUESTION = "è¿™å¼ å¤„æ–¹ä¸Šå†™äº†ä»€ä¹ˆï¼Ÿ å°¤å…¶æ˜¯è¯å“ã€æœç”¨æ¬¡æ•°ç­‰ï¼Œè¯·å‡†ç¡®å…¨éƒ¨å‘Šè¯‰æˆ‘ã€‚" 
tools = [vqa_model]

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

QWEN_MODEL = None
QWEN_PROCESSOR = None

# 3. ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ í•¨ìˆ˜ (ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ë‹¨ í•œ ë²ˆ í˜¸ì¶œ)
def load_qwen_components(model_name: str, device: str):
    """Qwen2VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ì „ì—­ìœ¼ë¡œ ë¡œë“œ"""
    global QWEN_MODEL, QWEN_PROCESSOR, DEVICE
    
    if QWEN_MODEL is not None:
        logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸš€ VQA ëª¨ë¸ '{model_name}' ì „ì—­ ë¡œë“œ ì‹œì‘ (Device: {device})...")
    torch_dtype = torch.float16 if device == "cuda" else torch.float32
    device_map = "auto" if device == "cuda" else None

    try:
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        # âš ï¸ Qwen2VLForConditionalGeneration í´ë˜ìŠ¤ ê²½ë¡œëŠ” ì‚¬ìš© í™˜ê²½ì— ë§ê²Œ ì¡°ì • í•„ìš”
        QWEN_MODEL = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            device_map=device_map
        )
        QWEN_MODEL.eval()
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        QWEN_PROCESSOR = AutoProcessor.from_pretrained(model_name)
        logger.info("âœ… ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ì „ì—­ ë¡œë“œ ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"âŒ VQA ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

# 4. VQA íˆ´ ì •ì˜ (ì „ì—­ ëª¨ë¸ ì‚¬ìš© ë° ì¸ì í™œìš©)
# âš ï¸ process_vision_info ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ê°€ ì „ì—­ìœ¼ë¡œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨
@tool
def vqa_model(image_path: str, question: str = FIXED_QUESTION, max_new_tokens: int = 128) -> str:
    """
    ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œì™€ í…ìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, Qwen2VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (Visual Question Answering)
    """
    global QWEN_MODEL, QWEN_PROCESSOR, DEVICE
    
    if QWEN_MODEL is None or QWEN_PROCESSOR is None:
        return "ì˜¤ë¥˜: Qwen2VL ëª¨ë¸ì´ ì „ì—­ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_qwen_componentsë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
        
    try:
        # 2. ë©”ì‹œì§€ êµ¬ì„± (í•¨ìˆ˜ ì¸ìë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •)
        messages = [
            {
                "role": "user",
                "content": [
                    # âš ï¸ type: "image", image: image_path í¬ë§·ì€ Qwen2VLì— ë§ê²Œ ì¡°ì • í•„ìš”
                    {"type": "image", "image": image_path}, 
                    {"type": "text", "text": f"<image>\n{question}"}
                ],
            }
        ]
        
        logger.info(f"ğŸ–¼ï¸ VQA ì¶”ë¡  ì‹œì‘ (Image: {image_path}) - Question: {question[:50]}...")
        
        # 3. processorë¡œ ì…ë ¥ ì¤€ë¹„ (ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ ë¡œì§ ìœ ì§€)
        print("ì…ë ¥ í…ì„œ ì¤€ë¹„ ì¤‘...")
        text_input = QWEN_PROCESSOR.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        # âš ï¸ process_vision_info í•¨ìˆ˜ëŠ” Qwen2VL ì‚¬ìš© í™˜ê²½ì— ì •ì˜ë˜ì–´ì•¼ í•¨
        image_inputs, _ = process_vision_info(messages) 

        inputs = QWEN_PROCESSOR(
            text=[text_input],
            images=image_inputs,
            padding=True,
            return_tensors="pt"
        ).to(DEVICE)

        # 4. ì¶”ë¡ 
        with torch.no_grad():
            generated_ids = QWEN_MODEL.generate(**inputs, max_new_tokens=max_new_tokens)

        # ìƒì„±ëœ í† í° ì¶”ì¶œ ë° ë””ì½”ë”©
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]

        output_text = QWEN_PROCESSOR.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        logger.info("âœ… VQA ì¶”ë¡  ì™„ë£Œ.")
        return output_text[0]

    except FileNotFoundError:
        return f"ì˜¤ë¥˜: ì§€ì •ëœ ì´ë¯¸ì§€ ê²½ë¡œ '{image_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"VQA ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a specialized assistant that uses the vqa_model tool for visual questions."), 
    ("human", "{input}"), 
    ("placeholder", "{agent_scratchpad}"),
])

llm = HuggingFacePipeline.from_model_id( # ChatHuggingFace
    model_id=HF_REPO_ID,
    task="text-generation",
    model_kwargs={"temperature": 0.1, "max_length": 512}
)

agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

agent_executor.invoke({"input": f"ì´ë¯¸ì§€ {IMAGE_URL}ì— ë¬´ì—‡ì´ ì“°ì—¬ ìˆëŠ”ì§€ í™•ì¸í•´ ì¤˜." })