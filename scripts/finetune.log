[2025-10-10 23:52:22,382] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-10 23:52:24,135] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-10 23:52:24,140] [WARNING] [runner.py:220:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2025-10-10 23:52:24,140] [INFO] [runner.py:610:main] cmd = /home/jwlee/miniconda3/envs/train/bin/python3.11 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None src/train/train_sft.py --use_liger True --lora_enable True --use_dora False --lora_namespan_exclude ['lm_head', 'embed_tokens'] --lora_rank 32 --lora_alpha 32 --lora_dropout 0.05 --num_lora_modules -1 --deepspeed scripts/zero3_offload.json --model_id Qwen/Qwen2-VL-2B-Instruct --data_path synth_rx/train_cord.json --image_folder data/cord_sample/images --remove_unused_columns False --freeze_vision_tower False --freeze_llm True --freeze_merger False --bf16 True --fp16 False --disable_flash_attn2 False --output_dir /home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord --num_train_epochs 2 --per_device_train_batch_size 2 --gradient_accumulation_steps 8 --image_min_pixels 200704 --image_max_pixels 1097600 --learning_rate 1e-4 --merger_lr 1e-5 --vision_lr 2e-6 --weight_decay 0.1 --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --gradient_checkpointing True --report_to tensorboard --lazy_preprocess True --save_strategy steps --save_steps 50 --save_total_limit 5 --dataloader_num_workers 2
[2025-10-10 23:52:25,280] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-10 23:52:27,049] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-10-10 23:52:27,053] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-10-10 23:52:27,053] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-10-10 23:52:27,053] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-10-10 23:52:27,053] [INFO] [launch.py:164:main] dist_world_size=2
[2025-10-10 23:52:27,053] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-10-10 23:52:27,054] [INFO] [launch.py:256:main] process 59557 spawned with command: ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', 'src/train/train_sft.py', '--local_rank=0', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', 'scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', 'synth_rx/train_cord.json', '--image_folder', 'data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2']
[2025-10-10 23:52:27,055] [INFO] [launch.py:256:main] process 59558 spawned with command: ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', 'src/train/train_sft.py', '--local_rank=1', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', 'scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', 'synth_rx/train_cord.json', '--image_folder', 'data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2']
/home/jwlee/miniconda3/envs/train/bin/python3.11: can't open file '/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/src/train/train_sft.py': [Errno 2] No such file or directory
/home/jwlee/miniconda3/envs/train/bin/python3.11: can't open file '/home/jwlee/volume/Qwen2-vl-finetune-wo/scripts/src/train/train_sft.py': [Errno 2] No such file or directory
[2025-10-10 23:52:28,055] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59557
[2025-10-10 23:52:28,063] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59558
[2025-10-10 23:52:28,063] [ERROR] [launch.py:325:sigkill_handler] ['/home/jwlee/miniconda3/envs/train/bin/python3.11', '-u', 'src/train/train_sft.py', '--local_rank=1', '--use_liger', 'True', '--lora_enable', 'True', '--use_dora', 'False', '--lora_namespan_exclude', "['lm_head', 'embed_tokens']", '--lora_rank', '32', '--lora_alpha', '32', '--lora_dropout', '0.05', '--num_lora_modules', '-1', '--deepspeed', 'scripts/zero3_offload.json', '--model_id', 'Qwen/Qwen2-VL-2B-Instruct', '--data_path', 'synth_rx/train_cord.json', '--image_folder', 'data/cord_sample/images', '--remove_unused_columns', 'False', '--freeze_vision_tower', 'False', '--freeze_llm', 'True', '--freeze_merger', 'False', '--bf16', 'True', '--fp16', 'False', '--disable_flash_attn2', 'False', '--output_dir', '/home/jwlee/volume/Qwen2-vl-finetune-wo/output/qwen2vl_cord', '--num_train_epochs', '2', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8', '--image_min_pixels', '200704', '--image_max_pixels', '1097600', '--learning_rate', '1e-4', '--merger_lr', '1e-5', '--vision_lr', '2e-6', '--weight_decay', '0.1', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--gradient_checkpointing', 'True', '--report_to', 'tensorboard', '--lazy_preprocess', 'True', '--save_strategy', 'steps', '--save_steps', '50', '--save_total_limit', '5', '--dataloader_num_workers', '2'] exits with return code = 2
